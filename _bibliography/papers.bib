
@string{aps = {American Physical Society,}}

@INPROCEEDINGS{bmvc,
  author={Alzugaray, Ignacio and Chli, Margarita},
  booktitle={British Machine Vision Virtual Conference (BMVC)}, 
  title={HASTE: multi-Hypothesis Asynchronous Speeded-up Tracking of Events}, 
  year={2020},
  volume={},
  number={},
  abstract={Feature tracking using event cameras has experienced significant progress lately, with methods achieving comparable performance to feature trackers using traditional frame-based cameras, even outperforming them on certain challenging scenarios. Most of the event-based trackers, however, still operate on intermediate, frame-like representations generated from accumulated events, on which traditional frame-based techniques can be adopted. Attempting to harness the sparsity and asynchronicity of the event stream, other approaches have emerged to process each event individually, but they lack both in accuracy and efficiency in comparison to the event-based, frame-like alternatives. Aiming to address this shortcoming of asynchronous approaches, in this paper, we propose an asynchronous patch-feature tracker that relies solely on events and processes each event individually as soon as it gets generated. We report significant improvements in tracking quality over the state of the art in publicly available datasets, while performing an order of magnitude more efficiently than similar asynchronous tracking approaches.},
  keywords={},
  month={Sep.},
html={https://www.bmvc2020-conference.com/conference/papers/paper_0744.html},
pdf={https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/439297/1/BMVC2020_HASTE_0744.pdf},
selected={true},
abbr={BMVC},}

@article{LAI2020106905,
title = {Full-field structural monitoring using event cameras and physics-informed sparse identification},
journal = {Mechanical Systems and Signal Processing},
volume = {145},
pages = {106905},
year = {2020},
issn = {0888-3270},
doi = {https://doi.org/10.1016/j.ymssp.2020.106905},
url = {https://www.sciencedirect.com/science/article/pii/S0888327020302910},
author = {Zhilu Lai and Ignacio Alzugaray and Margarita Chli and Eleni Chatzi},
keywords = {Vision-based monitoring, Physics-informed data science, Boundary condition learning, Event camera, Strain estimation, Structural health monitoring},
abstract = {This paper exploits a new direction of full-field structural monitoring and vibration analysis, using an emerging type of neuro-inspired vision sensors – namely event cameras. Compared to traditional frame-based cameras, event cameras offer salient benefits of resilience to motion blur, high dynamic range, and microsecond latency. Event cameras are herein exploited for structural monitoring, in order to extract dense measurements of structural response in terms of both spatial and temporal resolution. The output of an event camera is a stream of so called “events”, which is different to traditional snapshots. Due to this fundamentally different working principle, basic computer vision algorithms, such as optical flow or feature tracking, should be re-designed for processing event-based measurements. In this work, we present a novel framework termed physics-informed sparse identification, for full-field structural vibration tracking and analysis. The framework leverages sparse identification guided by assimilation of the underlying structural dynamics in the assembly of a library matrix, which is used to characterize the system’s dynamics. The stream of event data generated from event cameras is sparsely represented by means of well-chosen basis functions, allowing for a physical interpretation of the system’s response. The proposed framework is extended to boundary condition learning/classification by fusion of characteristic basis functions, representing different classes of support conditions, into the library matrix. The results obtained by means of an illustrative numerical example, as well as experimental tests on vibrating beams recorded by an event camera demonstrate an accurate tracking of structural vibration and the developed strains, in the form of full-field measurements rather than point-wise tracking. What is more, the proposed sparse learning process enables identification of the boundary conditions of monitored structural elements, which comes with key benefits for structural monitoring.},
html={https://doi.org/10.1016/j.ymssp.2020.106905},
selected={false},
abbr={MSSP},
}


@INPROCEEDINGS{9341208,
  author={Le Gentil, Cedric and Tschopp, Florian and Alzugaray, Ignacio and Vidal-Calleja, Teresa and Siegwart, Roland and Nieto, Juan},
  booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)}, 
  title={IDOL: A Framework for IMU-DVS Odometry using Lines}, 
  year={2020},
  volume={},
  number={},
  pages={5863-5870},
  abstract={In this paper, we introduce IDOL, an optimization-based framework for IMU-DVS Odometry using Lines. Event cameras, also called Dynamic Vision Sensors (DVSs), generate highly asynchronous streams of events triggered upon illumination changes for each individual pixel. This novel paradigm presents advantages in low illumination conditions and high-speed motions. Nonetheless, this unconventional sensing modality brings new challenges to perform scene reconstruction or motion estimation. The proposed method offers to leverage a continuous-time representation of the inertial readings to associate each event with timely accurate inertial data. The method's front-end extracts event clusters that belong to line segments in the environment whereas the back-end estimates the system's trajectory alongside the lines' 3D position by minimizing point-to-line distances between individual events and the lines' projection in the image space. A novel attraction/repulsion mechanism is presented to accurately estimate the lines' extremities, avoiding their explicit detection in the event data. The proposed method is benchmarked against a state-of-the-art frame-based visual-inertial odometry framework using public datasets. The results show that IDOL performs at the same order of magnitude on most datasets and even shows better orientation estimates. These findings can have a great impact on new algorithms for DVS.},
  keywords={},
  doi={10.1109/IROS45743.2020.9341208},
  ISSN={2153-0866},
  month={Oct},
html={https://ieeexplore.ieee.org/document/9341208},
pdf={https://arxiv.org/pdf/2008.05749.pdf},
selected={false},
abbr={IROS},
}


@INPROCEEDINGS{8885542,
  author={Alzugaray, Ignacio and Chli, Margarita},
  booktitle={International Conference on 3D Vision (3DV)}, 
  title={Asynchronous Multi-Hypothesis Tracking of Features with Event Cameras}, 
  year={2019},
  volume={},
  number={},
  pages={269-278},
  abstract={With the emergence of event cameras, increasing research effort has been focusing on processing the asynchronous stream of events. With each event encoding a discrete intensity change at a particular pixel, uniquely time-stamped with high accuracy, this sensing information is so fundamentally different to the data provided by traditional frame-based cameras that most of the well-established vision algorithms are not applicable. Inspired by the need of effective event-based tracking, this paper addresses the tracking of generic patch features relying solely on events, while exploiting their asynchronicity and high-temporal resolution. The proposed approach outperforms the state-of-the-art in event-based feature tracking on well-established event camera datasets, retrieving longer and more accurate feature tracks at higher a frequency. Considering tracking as an optimization problem of matching the current view to a feature template, the proposed method implements a simple and efficient technique that only requires the evaluation of a discrete set of tracking hypotheses.},
  keywords={},
  doi={10.1109/3DV.2019.00038},
  ISSN={2475-7888},
  month={Sep.},
html={https://ieeexplore.ieee.org/document/8885542},
pdf={https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/380462.1/3DV2019.pdf},
selected={false},
abbr={3DV},}


@INPROCEEDINGS{8491018,
  author={Alzugaray, Ignacio and Chli, Margarita},
  booktitle={International Conference on 3D Vision (3DV)}, 
  title={ACE: An Efficient Asynchronous Corner Tracker for Event Cameras}, 
  year={2018},
  volume={},
  number={},
  pages={653-661},
  abstract={The emergence of bio-inspired event cameras has opened up new exciting possibilities in high-frequency tracking, overcoming some of the limitations of traditional frame-based vision (e.g. motion blur during high-speed motions or saturation in scenes with high dynamic range). As a result, research has been focusing on the processing of their unusual output: an asynchronous stream of events. With the majority of existing techniques discretizing the event-stream into frame-like representations, we are yet to harness the true power of these cameras. In this paper, we propose the ACE tracker: a purely asynchronous framework to track corner-event features. Evaluation on benchmarking datasets reveals significant improvements in accuracy and computational efficiency in comparison to state-of-the-art event-based trackers. ACE achieves robust performance even in challenging scenarios, where traditional frame-based vision algorithms fail.},
  keywords={},
  doi={10.1109/3DV.2018.00080},
  ISSN={2475-7888},
  month={Sep.},
html={https://ieeexplore.ieee.org/document/8491018},
pdf={https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/308013/3DV2018.pdf},
selected={true},
abbr={3DV},}

@ARTICLE{8392795,
  author={Alzugaray, Ignacio and Chli, Margarita},
  journal={IEEE Robotics and Automation Letters}, 
  title={Asynchronous Corner Detection and Tracking for Event Cameras in Real Time}, 
  year={2018},
  volume={3},
  number={4},
  pages={3177-3184},
  abstract={The recent emergence of bioinspired event cameras has opened up exciting new possibilities in high-frequency tracking, bringing robustness to common problems in traditional vision, such as lighting changes and motion blur. In order to leverage these attractive attributes of the event cameras, research has been focusing on understanding how to process their unusual output: an asynchronous stream of events. With the majority of existing techniques discretizing the event-stream essentially forming frames of events grouped according to their timestamp, we are still to exploit the power of these cameras. In this spirit, this letter proposes a new, purely event-based corner detector, and a novel corner tracker, demonstrating that it is possible to detect corners and track them directly on the event stream in real time. Evaluation on benchmarking datasets reveals a significant boost in the number of detected corners and the repeatability of such detections over the state of the art even in challenging scenarios with the proposed approach while enabling more than a 4× speed-up when compared to the most efficient algorithm in the literature. The proposed pipeline detects and tracks corners at a rate of more than 7.5 million events per second, promising great impact in high-speed applications.},
  keywords={},
  doi={10.1109/LRA.2018.2849882},
  ISSN={2377-3766},
  month={Oct},
html={https://ieeexplore.ieee.org/document/8392795},
pdf={https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/277131/RAL2018-camera-ready.pdf},
selected={true},
abbr={RA-L / IROS}
}

@INPROCEEDINGS{7989319,
  author={Alzugaray, Ignacio and Teixeira, Lucas and Chli, Margarita},
  booktitle={IEEE International Conference on Robotics and Automation (ICRA)}, 
  title={Short-term UAV path-planning with monocular-inertial SLAM in the loop}, 
  year={2017},
  volume={},
  number={},
  pages={2739-2746},
  abstract={Small Unmanned Aerial Vehicles (UAVs) are some of the most promising robotic platforms in a variety of applications due to their high mobility. Their restricted computational and payload capabilities, however, translate into significant challenges in automating their navigation. With Simultaneous Localization And Mapping (SLAM) systems recently demonstrated to be employable onboard UAVs, the focus fall on path-planning on the quest of achieving autonomous navigation. With the vast body of path-planning literature often assuming perfect maps or maps known a priori, the biggest challenge lies in dealing with the robustness and accuracy limitations of onboard SLAM in real missions. In this spirit, this paper proposes a path-planning algorithm designed to work in the loop of the SLAM estimation of a monocular-inertial system. This point-to-point planner is demonstrated to navigate in an unknown environment using the incrementally generated SLAM map, while dictating the navigation strategy for preferable acquisition of sensor data for better estimations within SLAM. A thorough evaluation testbed of both simulated and real data is presented, demonstrating the robustness of the proposed pipeline against the state-of-the-art and its dramatically lower computational complexity, revealing its suitability to UAV navigation.},
  keywords={},
  doi={10.1109/ICRA.2017.7989319},
  ISSN={},
  month={May},
html={https://ieeexplore.ieee.org/document/7989319},
pdf={https://www.research-collection.ethz.ch/bitstream/handle/20.500.11850/235995.1/eth-50627-01.pdf?sequence=1},
selected={true},
abbr={ICRA}
}





@INPROCEEDINGS{7759257,  
author={Alzugaray, Ignacio and Sanfeliu, Alberto}, 
booktitle={IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},   title={Learning the hidden human knowledge of UAV pilots when navigating in a cluttered environment for improving path planning},   
year={2016},  
volume={},  
number={},  
pages={1589-1594},  
abstract={We propose in this work a new model of how the hidden human knowledge (HHK) of UAV pilots can be incorporated in the UAVs path planning generation. We intuitively know that human's pilots barely manage or even attempt to drive the UAV through a path that is optimal attending to some criteria as an optimal planner would suggest. Although human pilots might get close but not reach the optimal path proposed by some planner that optimizes over time or distance, the final effect of this differentiation could be not only surprisingly better, but also desirable. In the best scenario for optimality, the path that human pilots generate would deviate from the optimal path as much as the hidden knowledge that its perceives is injected into the path. The aim of our work is to use real human pilot paths to learn the hidden knowledge using repulsion fields and to incorporate this knowledge afterwards in the environment obstacles as cause of the deviation from optimality. We present a strategy of learning this knowledge based on attractor and repulsors, the learning method and a modified RRT* that can use this knowledge for path planning. Finally we do real-life tests and we compare the resulting paths with and without this knowledge.},  
keywords={},  
doi={10.1109/IROS.2016.7759257},  
ISSN={2153-0866},  
month={Oct},
pdf={http://www.iri.upc.edu/files/scidoc/1779-Learning-the-Hidden-Human-Knowledge-of-UAV-Pilots-when-navigating-in-a-cluttered-environment-for-improving-Path-Planning.pdf},
html={https://ieeexplore.ieee.org/document/7759257},
abbr={IROS}
}


@InProceedings{10.1007/978-3-319-67361-5_13,
author="Teixeira, Lucas
and Alzugaray, Ignacio
and Chli, Margarita",
editor="Hutter, Marco
and Siegwart, Roland",
title="Autonomous Aerial Inspection Using Visual-Inertial Robust Localization and Mapping",
booktitle="Field and Service Robotics",
year="2018",
publisher="Springer International Publishing",
address="Cham",
pages="191--204",
abstract="With recent technological breakthroughs bringing fully autonomous inspection using small Unmanned Aerial Vehicles (UAVs) closer to reality, the community of Robotics has actively been developing the real-time perception capabilities able to run onboard such constraint platforms. Despite good progress, realistic deployment of autonomous UAVs in GPS-denied environments is still rudimentary. In this work, we propose a novel system to generate a collision-free path towards a user-specified inspection direction for a small UAV using monocular-inertial sensing only and performing all computation onboard. Estimating both the previously unknown scene and the UAV's trajectory on the fly, this system is evaluated on real experiments outdoors in the presence of wind and poorly structured environments. Our analysis reveals the shortcomings of using sparse feature maps for planning, highlighting the importance of robust dense scene estimation proposed here.",
isbn="978-3-319-67361-5",
html={https://doi.org/10.1007/978-3-319-67361-5_13},
pdf={http://www.margaritachli.com/papers/FSR2017paper.pdf},
selected={false},
abbr={FSR},
}







